/*
 * Multi-GPU FlashAttention CUDA Implementation
 * 
 * This extends the advanced single-GPU kernel to a multi-GPU scenario using:
 * - Host-side threads for GPU management
 * - NCCL for inter-device communication
 * - Query partitioning across GPUs
 * - Novel fused online softmax with cooperative groups
 */

#include <cuda.h>
#include <cuda_runtime.h>
#include <nccl.h>
#include <cooperative_groups.h>
#include <thread>
#include <vector>
#include <iostream>
#include <cmath>

namespace cg = cooperative_groups;

// Constants
constexpr int WARP_SIZE = 32;
constexpr int TILE_K = 64;
constexpr int BLOCK_SIZE = 256;

// Advanced FlashAttention kernel (same as single GPU version)
template<typename T>
__global__ void multi_gpu_flash_attention_kernel(
    const T* __restrict__ Q,    // [local_queries, d_model]
    const T* __restrict__ K,    // [global_seq_len, d_model]
    const T* __restrict__ V,    // [global_seq_len, d_model]
    T* __restrict__ O,          // [local_queries, d_model]
    float* __restrict__ L,      // [local_queries] log-sum-exp
    int local_queries,
    int global_seq_len,
    int d_model,
    float scale
) {
    // Thread block and warp setup
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<WARP_SIZE> warp = cg::tiled_partition<WARP_SIZE>(block);
    
    const int tid = threadIdx.x;
    const int query_idx = blockIdx.x;
    
    if (query_idx >= local_queries) return;
    
    // Shared memory for query caching
    extern __shared__ T smem_q[];
    
    // Load query to shared memory
    for (int d = tid; d < d_model; d += BLOCK_SIZE) {
        smem_q[d] = Q[query_idx * d_model + d];
    }
    block.sync();
    
    // Initialize accumulators for online softmax
    float acc_num[4] = {0.0f};  // Assuming d_model <= 1024
    float acc_den = 0.0f;
    float running_max = -INFINITY;
    
    // Process keys/values in tiles
    int num_tiles = (global_seq_len + TILE_K - 1) / TILE_K;
    
    for (int tile = 0; tile < num_tiles; tile++) {
        int tile_start = tile * TILE_K;
        
        // Process each key in the tile
        for (int k = 0; k < TILE_K; k++) {
            int key_idx = tile_start + k;
            if (key_idx >= global_seq_len) break;
            
            // Compute dot product Q[query_idx] Â· K[key_idx]
            float dot = 0.0f;
            for (int d = tid; d < d_model; d += BLOCK_SIZE) {
                dot += smem_q[d] * K[key_idx * d_model + d];
            }
            
            // Reduce within warp
            dot = cg::reduce(warp, dot, cg::plus<float>());
            
            // Broadcast to all threads
            dot = cg::shfl(warp, dot, 0);
            
            // Scale
            dot *= scale;
            
            // Online softmax update
            float new_max = fmaxf(running_max, dot);
            float exp_factor = expf(running_max - new_max);
            float exp_val = expf(dot - new_max);
            
            // Update accumulators with value vector
            for (int d = tid; d < d_model; d += BLOCK_SIZE) {
                int acc_idx = (d * 4) / d_model;  // Map to accumulator
                float v_val = V[key_idx * d_model + d];
                acc_num[acc_idx] = acc_num[acc_idx] * exp_factor + v_val * exp_val;
            }
            
            // Update denominator and max
            if (tid == 0) {
                acc_den = acc_den * exp_factor + exp_val;
                running_max = new_max;
            }
        }
        
        // Sync within block
        block.sync();
    }
    
    // Broadcast final values
    acc_den = cg::shfl(warp, acc_den, 0);
    
    // Write output
    for (int d = tid; d < d_model; d += BLOCK_SIZE) {
        int acc_idx = (d * 4) / d_model;
        O[query_idx * d_model + d] = acc_num[acc_idx] / acc_den;
    }
    
    // Store log-sum-exp
    if (tid == 0 && L != nullptr) {
        L[query_idx] = running_max + logf(acc_den);
    }
}

// Structure to hold GPU-specific data
struct GPUContext {
    int device_id;
    cudaStream_t stream;
    
    // Device pointers
    float *d_Q, *d_K, *d_V, *d_O, *d_L;
    
    // Dimensions
    int local_queries;
    int global_seq_len;
    int d_model;
    
    // NCCL communicator
    ncclComm_t nccl_comm;
};

// Function to initialize a GPU
void init_gpu_context(GPUContext& ctx, int device_id, int local_queries, 
                     int global_seq_len, int d_model, ncclComm_t comm) {
    ctx.device_id = device_id;
    ctx.local_queries = local_queries;
    ctx.global_seq_len = global_seq_len;
    ctx.d_model = d_model;
    ctx.nccl_comm = comm;
    
    // Set device
    cudaSetDevice(device_id);
    
    // Create stream
    cudaStreamCreate(&ctx.stream);
    
    // Allocate device memory
    size_t size_q = local_queries * d_model * sizeof(float);
    size_t size_kv = global_seq_len * d_model * sizeof(float);
    size_t size_l = local_queries * sizeof(float);
    
    cudaMalloc(&ctx.d_Q, size_q);
    cudaMalloc(&ctx.d_K, size_kv);
    cudaMalloc(&ctx.d_V, size_kv);
    cudaMalloc(&ctx.d_O, size_q);
    cudaMalloc(&ctx.d_L, size_l);
}

// Function to run attention on a single GPU
void run_attention_on_gpu(GPUContext& ctx) {
    cudaSetDevice(ctx.device_id);
    
    // Configure kernel
    dim3 grid(ctx.local_queries);
    dim3 block(BLOCK_SIZE);
    size_t smem_size = ctx.d_model * sizeof(float);
    
    float scale = 1.0f / sqrtf(static_cast<float>(ctx.d_model));
    
    // Launch kernel
    multi_gpu_flash_attention_kernel<<<grid, block, smem_size, ctx.stream>>>(
        ctx.d_Q, ctx.d_K, ctx.d_V, ctx.d_O, ctx.d_L,
        ctx.local_queries, ctx.global_seq_len, ctx.d_model, scale
    );
}

// Multi-GPU FlashAttention class
class MultiGPUFlashAttention {
private:
    std::vector<GPUContext> gpu_contexts;
    std::vector<std::thread> gpu_threads;
    ncclComm_t* nccl_comms;
    int num_gpus;
    
public:
    MultiGPUFlashAttention(int num_gpus_) : num_gpus(num_gpus_) {
        // Initialize NCCL
        nccl_comms = new ncclComm_t[num_gpus];
        ncclCommInitAll(nccl_comms, num_gpus, nullptr);
        
        gpu_contexts.resize(num_gpus);
    }
    
    ~MultiGPUFlashAttention() {
        // Cleanup
        for (int i = 0; i < num_gpus; i++) {
            cudaSetDevice(i);
            cudaFree(gpu_contexts[i].d_Q);
            cudaFree(gpu_contexts[i].d_K);
            cudaFree(gpu_contexts[i].d_V);
            cudaFree(gpu_contexts[i].d_O);
            cudaFree(gpu_contexts[i].d_L);
            cudaStreamDestroy(gpu_contexts[i].stream);
            ncclCommDestroy(nccl_comms[i]);
        }
        delete[] nccl_comms;
    }
    
    void compute_attention(float* h_Q, float* h_K, float* h_V, float* h_O,
                          int total_queries, int global_seq_len, int d_model) {
        // Partition queries across GPUs
        int queries_per_gpu = total_queries / num_gpus;
        int remainder = total_queries % num_gpus;
        
        // Initialize GPU contexts
        int offset = 0;
        for (int i = 0; i < num_gpus; i++) {
            int local_queries = queries_per_gpu + (i < remainder ? 1 : 0);
            init_gpu_context(gpu_contexts[i], i, local_queries, 
                           global_seq_len, d_model, nccl_comms[i]);
            
            // Copy data to GPU
            cudaSetDevice(i);
            size_t size_q = local_queries * d_model * sizeof(float);
            size_t size_kv = global_seq_len * d_model * sizeof(float);
            
            cudaMemcpyAsync(gpu_contexts[i].d_Q, h_Q + offset * d_model, 
                          size_q, cudaMemcpyHostToDevice, gpu_contexts[i].stream);
            cudaMemcpyAsync(gpu_contexts[i].d_K, h_K, 
                          size_kv, cudaMemcpyHostToDevice, gpu_contexts[i].stream);
            cudaMemcpyAsync(gpu_contexts[i].d_V, h_V, 
                          size_kv, cudaMemcpyHostToDevice, gpu_contexts[i].stream);
            
            offset += local_queries;
        }
        
        // Launch kernels on all GPUs
        for (int i = 0; i < num_gpus; i++) {
            gpu_threads.emplace_back([this, i]() {
                run_attention_on_gpu(gpu_contexts[i]);
            });
        }
        
        // Wait for all GPUs to complete
        for (auto& thread : gpu_threads) {
            thread.join();
        }
        gpu_threads.clear();
        
        // Copy results back
        offset = 0;
        for (int i = 0; i < num_gpus; i++) {
            cudaSetDevice(i);
            int local_queries = gpu_contexts[i].local_queries;
            size_t size_o = local_queries * d_model * sizeof(float);
            
            cudaMemcpyAsync(h_O + offset * d_model, gpu_contexts[i].d_O,
                          size_o, cudaMemcpyDeviceToHost, gpu_contexts[i].stream);
            offset += local_queries;
        }
        
        // Synchronize all streams
        for (int i = 0; i < num_gpus; i++) {
            cudaSetDevice(i);
            cudaStreamSynchronize(gpu_contexts[i].stream);
        }
    }
    
    // Optional: All-gather outputs across GPUs using NCCL
    void all_gather_outputs() {
        ncclGroupStart();
        
        for (int i = 0; i < num_gpus; i++) {
            cudaSetDevice(i);
            // Implementation of all-gather using NCCL
            // This would gather all outputs to all GPUs
        }
        
        ncclGroupEnd();
    }
};

// Example usage
int main() {
    // Check available GPUs
    int num_gpus;
    cudaGetDeviceCount(&num_gpus);
    std::cout << "Found " << num_gpus << " GPUs" << std::endl;
    
    if (num_gpus < 2) {
        std::cerr << "This example requires at least 2 GPUs" << std::endl;
        return 1;
    }
    
    // Problem dimensions
    const int total_queries = 4096;
    const int global_seq_len = 4096;
    const int d_model = 128;
    
    // Allocate host memory
    size_t size = total_queries * d_model * sizeof(float);
    float *h_Q = new float[total_queries * d_model];
    float *h_K = new float[global_seq_len * d_model];
    float *h_V = new float[global_seq_len * d_model];
    float *h_O = new float[total_queries * d_model];
    
    // Initialize with random data
    for (int i = 0; i < total_queries * d_model; i++) {
        h_Q[i] = static_cast<float>(rand()) / RAND_MAX;
    }
    for (int i = 0; i < global_seq_len * d_model; i++) {
        h_K[i] = static_cast<float>(rand()) / RAND_MAX;
        h_V[i] = static_cast<float>(rand()) / RAND_MAX;
    }
    
    // Create multi-GPU attention
    MultiGPUFlashAttention attention(num_gpus);
    
    // Run attention
    auto start = std::chrono::high_resolution_clock::now();
    attention.compute_attention(h_Q, h_K, h_V, h_O, 
                              total_queries, global_seq_len, d_model);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    std::cout << "Multi-GPU attention completed in " << duration.count() << " ms" << std::endl;
    
    // Verify results (check first few outputs)
    std::cout << "\nFirst 5 outputs:" << std::endl;
    for (int i = 0; i < 5; i++) {
        std::cout << "Query " << i << ": ";
        for (int j = 0; j < 5; j++) {
            std::cout << h_O[i * d_model + j] << " ";
        }
        std::cout << "..." << std::endl;
    }
    
    // Cleanup
    delete[] h_Q;
    delete[] h_K;
    delete[] h_V;
    delete[] h_O;
    
    return 0;
}
