/*
 * Advanced FlashAttention CUDA Implementation with Double Buffering
 * 
 * This implements the novel techniques described in the documentation:
 * - Fused online softmax update with running accumulators
 * - Double buffering with cp.async for overlapping data transfer
 * - Cooperative groups for efficient reductions
 * - Tiled processing for memory efficiency
 */

#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
#include <cuda/pipeline>
#include <iostream>
#include <cmath>

namespace cg = cooperative_groups;

// Constants for kernel configuration
constexpr int WARP_SIZE = 32;
constexpr int TILE_K = 64;
constexpr int TILE_Q = 128;
constexpr int BLOCK_SIZE = 256;
constexpr int SMEM_STAGES = 2;  // Double buffering

// Helper function for cp.async with fallback
template<typename T>
__device__ __forceinline__ void async_copy_global_to_shared(
    T* smem_ptr, const T* gmem_ptr, bool pred) {
#if __CUDA_ARCH__ >= 800
    // Use cp.async for Ampere and newer
    if (pred) {
        __pipeline_memcpy_async(smem_ptr, gmem_ptr, sizeof(T));
    }
#else
    // Fallback for older architectures
    if (pred) {
        *smem_ptr = *gmem_ptr;
    }
#endif
}

// Advanced FlashAttention kernel with double buffering
template<typename T, int D_MODEL>
__global__ void advanced_flash_attention_kernel(
    const T* __restrict__ Q,    // [local_seq_len, d_model]
    const T* __restrict__ K,    // [global_seq_len, d_model]
    const T* __restrict__ V,    // [global_seq_len, d_model]
    T* __restrict__ O,          // [local_seq_len, d_model]
    float* __restrict__ L,      // [local_seq_len] log-sum-exp
    int local_seq_len,
    int global_seq_len,
    float scale
) {
    // Thread block and warp information
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<WARP_SIZE> warp = cg::tiled_partition<WARP_SIZE>(block);
    
    const int tid = threadIdx.x;
    const int wid = tid / WARP_SIZE;
    const int lane = tid % WARP_SIZE;
    const int query_idx = blockIdx.x;
    
    // Early exit if out of bounds
    if (query_idx >= local_seq_len) return;
    
    // Shared memory for double buffering
    extern __shared__ char smem[];
    T* smem_k = reinterpret_cast<T*>(smem);
    T* smem_v = smem_k + SMEM_STAGES * TILE_K * D_MODEL;
    T* smem_q = smem_v + SMEM_STAGES * TILE_K * D_MODEL;
    
    // Load query vector to shared memory (persistent across tiles)
    for (int d = tid; d < D_MODEL; d += BLOCK_SIZE) {
        smem_q[d] = Q[query_idx * D_MODEL + d];
    }
    block.sync();
    
    // Initialize accumulators for online softmax
    float acc_num[D_MODEL / BLOCK_SIZE + 1] = {0.0f};
    float acc_den = 0.0f;
    float running_max = -INFINITY;  // Robust initialization
    
    // Pipeline for asynchronous memory operations
#if __CUDA_ARCH__ >= 800
    cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();
#endif
    
    // Process keys/values in tiles with double buffering
    int num_tiles = (global_seq_len + TILE_K - 1) / TILE_K;
    
    for (int tile = 0; tile < num_tiles; tile++) {
        int tile_start = tile * TILE_K;
        int stage = tile % SMEM_STAGES;
        
        // Asynchronous load of K and V tiles
        for (int i = tid; i < TILE_K * D_MODEL; i += BLOCK_SIZE) {
            int k_idx = tile_start + i / D_MODEL;
            int d_idx = i % D_MODEL;
            
            bool in_bounds = (k_idx < global_seq_len) && (d_idx < D_MODEL);
            
            // Load K tile with cp.async
            async_copy_global_to_shared(
                &smem_k[stage * TILE_K * D_MODEL + i],
                &K[k_idx * D_MODEL + d_idx],
                in_bounds
            );
            
            // Load V tile with cp.async
            async_copy_global_to_shared(
                &smem_v[stage * TILE_K * D_MODEL + i],
                &V[k_idx * D_MODEL + d_idx],
                in_bounds
            );
        }
        
#if __CUDA_ARCH__ >= 800
        // Commit the async copies
        pipe.producer_commit();
        
        // Wait for the copies to complete
        pipe.consumer_wait();
#else
        block.sync();
#endif
        
        // Process the tile - compute QK^T
        for (int k = 0; k < TILE_K; k++) {
            int key_idx = tile_start + k;
            if (key_idx >= global_seq_len) break;
            
            // Compute dot product Q[query_idx] Â· K[key_idx]
            float dot = 0.0f;
            for (int d = tid; d < D_MODEL; d += BLOCK_SIZE) {
                dot += smem_q[d] * smem_k[stage * TILE_K * D_MODEL + k * D_MODEL + d];
            }
            
            // Reduce within warp using cooperative groups
            dot = cg::reduce(warp, dot, cg::plus<float>());
            
            // Broadcast to all threads in warp
            dot = cg::shfl(warp, dot, 0);
            
            // Scale the dot product
            dot *= scale;
            
            // Online softmax update (THE NOVEL PART!)
            float new_max = fmaxf(running_max, dot);
            float exp_factor = expf(running_max - new_max);
            float exp_val = expf(dot - new_max);
            
            // Update accumulators with current value vector
            for (int d = tid; d < D_MODEL; d += BLOCK_SIZE) {
                int acc_idx = d / BLOCK_SIZE;
                float v_val = smem_v[stage * TILE_K * D_MODEL + k * D_MODEL + d];
                acc_num[acc_idx] = acc_num[acc_idx] * exp_factor + v_val * exp_val;
            }
            
            // Update denominator and running max
            if (tid == 0) {
                acc_den = acc_den * exp_factor + exp_val;
                running_max = new_max;
            }
        }
        
        // Synchronize before next tile
        block.sync();
    }
    
    // Broadcast final acc_den and running_max to all threads
    acc_den = cg::shfl(warp, acc_den, 0);
    running_max = cg::shfl(warp, running_max, 0);
    
    // Compute final output: O = acc_num / acc_den
    for (int d = tid; d < D_MODEL; d += BLOCK_SIZE) {
        int acc_idx = d / BLOCK_SIZE;
        O[query_idx * D_MODEL + d] = acc_num[acc_idx] / acc_den;
    }
    
    // Store log-sum-exp for numerical stability checks
    if (tid == 0 && L != nullptr) {
        L[query_idx] = running_max + logf(acc_den);
    }
}

// Host function to launch the kernel
template<typename T>
void launch_advanced_flash_attention(
    const T* Q, const T* K, const T* V, T* O, float* L,
    int local_seq_len, int global_seq_len, int d_model,
    cudaStream_t stream = 0
) {
    // Calculate shared memory size for double buffering
    size_t smem_size = sizeof(T) * (2 * SMEM_STAGES * TILE_K * d_model + d_model);
    
    // Configure kernel launch parameters
    dim3 grid(local_seq_len);
    dim3 block(BLOCK_SIZE);
    
    // Set shared memory config for maximum shared memory
    cudaFuncSetAttribute(
        advanced_flash_attention_kernel<T, 128>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        smem_size
    );
    
    float scale = 1.0f / sqrtf(static_cast<float>(d_model));
    
    // Launch kernel based on d_model
    switch(d_model) {
        case 64:
            advanced_flash_attention_kernel<T, 64><<<grid, block, smem_size, stream>>>(
                Q, K, V, O, L, local_seq_len, global_seq_len, scale);
            break;
        case 128:
            advanced_flash_attention_kernel<T, 128><<<grid, block, smem_size, stream>>>(
                Q, K, V, O, L, local_seq_len, global_seq_len, scale);
            break;
        case 256:
            advanced_flash_attention_kernel<T, 256><<<grid, block, smem_size, stream>>>(
                Q, K, V, O, L, local_seq_len, global_seq_len, scale);
            break;
        default:
            std::cerr << "Unsupported d_model: " << d_model << std::endl;
            break;
    }
}

// Example usage and testing
int main() {
    // Dimensions
    const int local_seq_len = 1024;
    const int global_seq_len = 1024;
    const int d_model = 128;
    
    // Allocate device memory
    float *d_Q, *d_K, *d_V, *d_O, *d_L;
    size_t size_qo = local_seq_len * d_model * sizeof(float);
    size_t size_kv = global_seq_len * d_model * sizeof(float);
    size_t size_l = local_seq_len * sizeof(float);
    
    cudaMalloc(&d_Q, size_qo);
    cudaMalloc(&d_K, size_kv);
    cudaMalloc(&d_V, size_kv);
    cudaMalloc(&d_O, size_qo);
    cudaMalloc(&d_L, size_l);
    
    // Initialize with random data (in practice, copy from host)
    // ... initialization code ...
    
    // Launch kernel
    launch_advanced_flash_attention(
        d_Q, d_K, d_V, d_O, d_L,
        local_seq_len, global_seq_len, d_model
    );
    
    // Synchronize and check for errors
    cudaError_t err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        std::cerr << "CUDA error: " << cudaGetErrorString(err) << std::endl;
    }
    
    // Cleanup
    cudaFree(d_Q);
    cudaFree(d_K);
    cudaFree(d_V);
    cudaFree(d_O);
    cudaFree(d_L);
    
    return 0;
}
